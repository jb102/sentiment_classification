{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Classification of Tweets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix, csr_matrix\n",
    "from itertools import count\n",
    "from math import log\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sets\n",
    "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    readin the testset and return a dictionary\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training set, dev set and testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "sw = list(map(lambda a:a.lower(),stopwords.words('english')))\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub('\\n','', tweet)\n",
    "    for url_form in ['http://.*','https://.*']:\n",
    "        tweet = re.sub(' '+url_form, '', tweet)\n",
    "    tweet = re.sub('\\@[a-zA-Z0-9]+','', tweet)\n",
    "    tweet = re.sub('\\#[a-zA-Z0-9]+','', tweet)\n",
    "    tweet = re.sub('[^a-zA-Z0-9 ]', '', tweet)\n",
    "    tweet = re.sub('[0-9]+', '', tweet)\n",
    "    tweet = re.sub('\\b\\[a-zA-Z]b', '', tweet)\n",
    "    tweet = tweet.split()\n",
    "    for i,token in enumerate(tweet):\n",
    "        if i>0 and tweet[i-1].lower() in ['not', 'no', 'never']:\n",
    "            tweet[i] = 'not_'+tweet[i]\n",
    "    tweet = [word for word in map(lemmatizer.lemmatize,tweet) if word not in sw and len(word)>1]\n",
    "    tweet = ' '.join(tweet)\n",
    "    return tweet.lower()\n",
    "\n",
    "# Load training set, dev set and testing set\n",
    "data = {}\n",
    "tweetids = {}\n",
    "tweetgts = {}\n",
    "tweets = {}\n",
    "\n",
    "tweets_preprocessed = {}\n",
    "tweets_preprocessed_not_split = {}\n",
    "data_as_csr = {}\n",
    "\n",
    "for dataset in ['twitter-training-data.txt', 'twitter-dev-data.txt'] + testsets:\n",
    "    data[dataset] = []\n",
    "    tweets[dataset] = []\n",
    "    tweetids[dataset] = []\n",
    "    tweetgts[dataset] = []\n",
    "    \n",
    "    tweets_preprocessed[dataset] = []\n",
    "    tweets_preprocessed_not_split[dataset] = []\n",
    "\n",
    "    testset_path = join('semeval-tweets', dataset)\n",
    "    id_gts = {}\n",
    "    vocabulary = {}\n",
    "    indices = count()\n",
    "    with open(testset_path, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetids[dataset].append(fields[0])\n",
    "            tweetgts[dataset].append(fields[1])\n",
    "            tweets[dataset].append(fields[2])\n",
    "            \n",
    "            tweet_prep = preprocess_tweet(fields[2]).split()\n",
    "            for token in tweet_prep:\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary[token] = next(indices)\n",
    "            \n",
    "            tweets_preprocessed[dataset].append(tweet_prep)\n",
    "            tweets_preprocessed_not_split[dataset].append(preprocess_tweet(fields[2]))\n",
    "    fh.close()\n",
    "        \n",
    "    data[dataset] = dok_matrix\n",
    "                \n",
    "    #data_as_csr[dataset] = data[dataset].tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sentiment classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('subjclueslen1-HLTEMNLP05.tff','r',encoding='utf8')\n",
    "lexicons={}\n",
    "lexicons['positive']=set()\n",
    "lexicons['negative']=set()\n",
    "lexicons['neutral']=set()\n",
    "\n",
    "type_mapping = {'positive':'positive',\n",
    "               'negative':'negative',\n",
    "               'neutral':'neutral',\n",
    "               'both':'neutral',\n",
    "               'weakneg':'negative',\n",
    "               'trongneg':'negative'}\n",
    "\n",
    "for line in f:\n",
    "    l=line.split()\n",
    "    lexicons[type_mapping[l[-1][14:]]].add(l[2][6:])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_train(tweetgts, tweets, lexicon=False):\n",
    "    vocabulary_counts_positive['{lexicon}']=0\n",
    "    vocabulary_counts_negative['{lexicon}']=0\n",
    "    vocabulary_counts_neutral['{lexicon}']=0\n",
    "    for gts, tweet in zip(tweetgts, tweets):\n",
    "        #words_so_far = []\n",
    "        for token in tweet:\n",
    "            if lexicon:\n",
    "                if token in lexicons['positive']:\n",
    "                    vocabulary_counts_positive['{lexicon}']+=1\n",
    "                if token in lexicons['negative']:\n",
    "                    vocabulary_counts_negative['{lexicon}']+=1\n",
    "                if token in lexicons['neutral']:\n",
    "                    vocabulary_counts_neutral['{lexicon}']+=1\n",
    "            #if token in words_so_far:\n",
    "                #continue\n",
    "            #words_so_far.append(token)\n",
    "            if token not in vocabulary:\n",
    "                vocabulary.add(token)\n",
    "            if gts=='positive':\n",
    "                d = vocabulary_counts_positive\n",
    "            elif gts=='negative':\n",
    "                d = vocabulary_counts_negative\n",
    "            elif gts=='neutral':\n",
    "                d = vocabulary_counts_neutral\n",
    "            else:\n",
    "                print(\"problem\")\n",
    "            try:\n",
    "                d[token]+=1\n",
    "            except KeyError:\n",
    "                d[token]=1\n",
    "\n",
    "def bayes_n_doc(c):\n",
    "    return len([tc for tc in tweetgts['twitter-training-data.txt'] if tc == c])\n",
    "\n",
    "def bayes_log_prior(c):\n",
    "    return log(bayes_n_doc(c)/len(tweetgts['twitter-training-data.txt']),10)\n",
    "\n",
    "def bayes_count(w,c):\n",
    "    try:\n",
    "        if c=='positive':\n",
    "            return vocabulary_counts_positive[w]\n",
    "        if c=='negative':\n",
    "            return vocabulary_counts_negative[w]\n",
    "        if c=='neutral':\n",
    "            return vocabulary_counts_neutral[w]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "    \n",
    "def bayes_log_likelihood(w,c):\n",
    "    if c=='positive':\n",
    "        d=vocabulary_counts_positive\n",
    "    if c=='negative':\n",
    "        d=vocabulary_counts_negative\n",
    "    if c=='neutral':\n",
    "        d=vocabulary_counts_neutral\n",
    "    return log((bayes_count(w,c)+1)/(sum(d.values())+len(vocabulary)),10)\n",
    "\n",
    "def bayes_sum(tweet,c,lexicon):\n",
    "    s = bayes_log_prior(c)\n",
    "    for token in tweet:\n",
    "        if lexicon:\n",
    "            if token in lexicons['positive'] and c=='positive':\n",
    "                s = s + bayes_log_likelihood('{lexicon}',c)\n",
    "                continue\n",
    "            if token in lexicons['negative'] and c=='negative':\n",
    "                s = s + bayes_log_likelihood('{lexicon}',c)\n",
    "                continue\n",
    "            if token in lexicons['neutral'] and c=='neutral':\n",
    "                s = s + bayes_log_likelihood('{lexicon}',c)\n",
    "                continue\n",
    "        if token in vocabulary:\n",
    "            s = s + bayes_log_likelihood(token,c)\n",
    "    return s\n",
    "\n",
    "def bayes_predict(tweet,lexicon=False):\n",
    "    cat_key = {0:'positive',1:'negative',2:'neutral'}\n",
    "    likelihoods = [bayes_sum(tweet,c,lexicon) for c in ['positive','negative','neutral']]\n",
    "    return cat_key[likelihoods.index(max(likelihoods))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classes_for_svm(tweetgts):\n",
    "    return list(map(lambda a: 0 if a=='negative' else 1 if a=='neutral' else 2 if a=='positive' else None,tweetgts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training naive_bayes\n",
      "semeval-tweets\\twitter-dev-data.txt (bow-naive_bayes): 0.598\n",
      "semeval-tweets\\twitter-test1.txt (bow-naive_bayes): 0.486\n",
      "semeval-tweets\\twitter-test2.txt (bow-naive_bayes): 0.468\n",
      "semeval-tweets\\twitter-test3.txt (bow-naive_bayes): 0.485\n",
      "semeval-tweets\\twitter-dev-data.txt (bow+lexicons-naive_bayes): 0.562\n",
      "semeval-tweets\\twitter-test1.txt (bow+lexicons-naive_bayes): 0.512\n",
      "semeval-tweets\\twitter-test2.txt (bow+lexicons-naive_bayes): 0.506\n",
      "semeval-tweets\\twitter-test3.txt (bow+lexicons-naive_bayes): 0.508\n",
      "Training svm\n",
      "semeval-tweets\\twitter-dev-data.txt (svm): 0.577\n",
      "semeval-tweets\\twitter-test1.txt (svm): 0.498\n",
      "semeval-tweets\\twitter-test2.txt (svm): 0.522\n",
      "semeval-tweets\\twitter-test3.txt (svm): 0.495\n"
     ]
    }
   ],
   "source": [
    "svmoutputtoclass = lambda a: 'negative' if a==0 else 'neutral' if a==1 else 'positive' if a==2 else None\n",
    "\n",
    "for classifier in ['naive_bayes', 'svm']:\n",
    "\n",
    "    print('Training',classifier)\n",
    "        \n",
    "    if classifier=='naive_bayes':\n",
    "        #continue \n",
    "        for features in ['bow','bow+lexicons']:\n",
    "            for testset in ['twitter-dev-data.txt']+testsets:\n",
    "                id_preds = {}\n",
    "                vocabulary = set()\n",
    "                vocabulary_counts_positive = {}\n",
    "                vocabulary_counts_negative = {}\n",
    "                vocabulary_counts_neutral = {}\n",
    "                bayes_train(tweetgts['twitter-training-data.txt'],tweets_preprocessed['twitter-training-data.txt'],\n",
    "                            lexicon=True if features=='bow+lexicons' else False)\n",
    "                for tweetid,tweet in zip(tweetids[testset],tweets_preprocessed[testset]):\n",
    "                    id_preds[tweetid] = bayes_predict(tweet,lexicon=True if features=='bow+lexicons' else False)   \n",
    "                testset_name = testset\n",
    "                testset_path = join('semeval-tweets', testset_name)\n",
    "                evaluate(id_preds, testset_path, features+'-'+classifier)\n",
    "            \n",
    "    if classifier=='svm':\n",
    "        #continue\n",
    "        vectorizer = TfidfVectorizer(min_df = 5,max_df = 0.8,sublinear_tf = True,use_idf = True)\n",
    "        train_vectors = vectorizer.fit_transform(tweets_preprocessed_not_split['twitter-training-data.txt'])\n",
    "        #uncomment this to build SVM model instead of loading from pickle\n",
    "        #clf = svm.SVC()\n",
    "        #clf.fit(train_vectors, classes_for_svm(tweetgts['twitter-training-data.txt']))\n",
    "        clf = pickle.load( open( \"svmmodel.p\", \"rb\" ) )\n",
    "        for testset in ['twitter-dev-data.txt']+testsets:\n",
    "            id_preds = {}\n",
    "            test_vectors = vectorizer.transform(tweets_preprocessed_not_split[testset])\n",
    "            dev_set_predictions = clf.predict(test_vectors)\n",
    "            for i in range(len(tweetids[testset])):              \n",
    "                id_preds[tweetids[testset][i]] = svmoutputtoclass(dev_set_predictions[i])\n",
    "    \n",
    "            testset_name = testset\n",
    "            testset_path = join('semeval-tweets', testset_name)\n",
    "            evaluate(id_preds, testset_path, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Classification of Tweets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix, csr_matrix\n",
    "from itertools import count\n",
    "from math import log\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test sets\n",
    "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton: Evaluation code for the test sets\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    readin the testset and return a dictionary\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training set, dev set and testing set\n",
    "Here, you need to load the training set, the development set and the test set. For better classification results, you may need to preprocess tweets before sending them to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "sw = list(map(lambda a:a.lower(),stopwords.words('english')))\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = re.sub('\\n','', tweet)\n",
    "    for url_form in ['http://.*','https://.*']:\n",
    "        tweet = re.sub(' '+url_form, '', tweet)\n",
    "    tweet = re.sub('\\@[a-zA-Z0-9]+','', tweet)\n",
    "    tweet = re.sub('\\#[a-zA-Z0-9]+','', tweet)\n",
    "    tweet = re.sub('[^a-zA-Z0-9 ]', '', tweet)\n",
    "    tweet = re.sub('[0-9]+', '', tweet)\n",
    "    tweet = re.sub('\\b\\[a-zA-Z]b', '', tweet)\n",
    "    tweet = tweet.split()\n",
    "    for i,token in enumerate(tweet):\n",
    "        if i>0 and tweet[i-1].lower() in ['not', 'no', 'never']:\n",
    "            tweet[i] = 'not_'+tweet[i]\n",
    "    tweet = [word for word in map(lemmatizer.lemmatize,tweet) if word not in sw and len(word)>1]\n",
    "    tweet = ' '.join(tweet)\n",
    "    return tweet.lower()\n",
    "\n",
    "# Load training set, dev set and testing set\n",
    "data = {}\n",
    "tweetids = {}\n",
    "tweetgts = {}\n",
    "tweets = {}\n",
    "\n",
    "tweets_preprocessed = {}\n",
    "tweets_preprocessed_not_split = {}\n",
    "data_as_csr = {}\n",
    "\n",
    "for dataset in ['twitter-training-data.txt', 'twitter-dev-data.txt'] + testsets:\n",
    "    data[dataset] = []\n",
    "    tweets[dataset] = []\n",
    "    tweetids[dataset] = []\n",
    "    tweetgts[dataset] = []\n",
    "    \n",
    "    tweets_preprocessed[dataset] = []\n",
    "    tweets_preprocessed_not_split[dataset] = []\n",
    "\n",
    "    testset_path = join('semeval-tweets', dataset)\n",
    "    id_gts = {}\n",
    "    vocabulary = {}\n",
    "    indices = count()\n",
    "    with open(testset_path, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetids[dataset].append(fields[0])\n",
    "            tweetgts[dataset].append(fields[1])\n",
    "            tweets[dataset].append(fields[2])\n",
    "            \n",
    "            tweet_prep = preprocess_tweet(fields[2]).split()\n",
    "            for token in tweet_prep:\n",
    "                if token not in vocabulary:\n",
    "                    vocabulary[token] = next(indices)\n",
    "            \n",
    "            tweets_preprocessed[dataset].append(tweet_prep)\n",
    "            tweets_preprocessed_not_split[dataset].append(preprocess_tweet(fields[2]))\n",
    "    fh.close()\n",
    "        \n",
    "    data[dataset] = dok_matrix\n",
    "                \n",
    "    #data_as_csr[dataset] = data[dataset].tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sentiment classifiers\n",
    "You need to create your own classifiers (at least 3 classifiers). For each classifier, you can choose between the bag-of-word features and the word-embedding-based features. Each classifier has to be evaluated over 3 test sets. Make sure your classifier produce consistent performance across the test sets. Marking will be based on the performance over all 5 test sets (2 of them are not provided to you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('subjclueslen1-HLTEMNLP05.tff','r',encoding='utf8')\n",
    "lexicons={}\n",
    "lexicons['positive']=set()\n",
    "lexicons['negative']=set()\n",
    "lexicons['neutral']=set()\n",
    "\n",
    "type_mapping = {'positive':'positive',\n",
    "               'negative':'negative',\n",
    "               'neutral':'neutral',\n",
    "               'both':'neutral',\n",
    "               'weakneg':'negative',\n",
    "               'trongneg':'negative'}\n",
    "\n",
    "for line in f:\n",
    "    l=line.split()\n",
    "    lexicons[type_mapping[l[-1][14:]]].add(l[2][6:])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_train(tweetgts, tweets, lexicon=False):\n",
    "    vocabulary_counts_positive['{lexicon}']=0\n",
    "    vocabulary_counts_negative['{lexicon}']=0\n",
    "    vocabulary_counts_neutral['{lexicon}']=0\n",
    "    for gts, tweet in zip(tweetgts, tweets):\n",
    "        #words_so_far = []\n",
    "        for token in tweet:\n",
    "            if lexicon:\n",
    "                if token in lexicons['positive']:\n",
    "                    vocabulary_counts_positive['{lexicon}']+=1\n",
    "                if token in lexicons['negative']:\n",
    "                    vocabulary_counts_negative['{lexicon}']+=1\n",
    "                if token in lexicons['neutral']:\n",
    "                    vocabulary_counts_neutral['{lexicon}']+=1\n",
    "            #if token in words_so_far:\n",
    "                #continue\n",
    "            #words_so_far.append(token)\n",
    "            if token not in vocabulary:\n",
    "                vocabulary.add(token)\n",
    "            if gts=='positive':\n",
    "                d = vocabulary_counts_positive\n",
    "            elif gts=='negative':\n",
    "                d = vocabulary_counts_negative\n",
    "            elif gts=='neutral':\n",
    "                d = vocabulary_counts_neutral\n",
    "            else:\n",
    "                print(\"problem\")\n",
    "            try:\n",
    "                d[token]+=1\n",
    "            except KeyError:\n",
    "                d[token]=1\n",
    "\n",
    "def bayes_n_doc(c):\n",
    "    return len([tc for tc in tweetgts['twitter-training-data.txt'] if tc == c])\n",
    "\n",
    "def bayes_log_prior(c):\n",
    "    return log(bayes_n_doc(c)/len(tweetgts['twitter-training-data.txt']),10)\n",
    "\n",
    "def bayes_count(w,c):\n",
    "    try:\n",
    "        if c=='positive':\n",
    "            return vocabulary_counts_positive[w]\n",
    "        if c=='negative':\n",
    "            return vocabulary_counts_negative[w]\n",
    "        if c=='neutral':\n",
    "            return vocabulary_counts_neutral[w]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "    \n",
    "def bayes_log_likelihood(w,c):\n",
    "    if c=='positive':\n",
    "        d=vocabulary_counts_positive\n",
    "    if c=='negative':\n",
    "        d=vocabulary_counts_negative\n",
    "    if c=='neutral':\n",
    "        d=vocabulary_counts_neutral\n",
    "    return log((bayes_count(w,c)+1)/(sum(d.values())+len(vocabulary)),10)\n",
    "\n",
    "def bayes_sum(tweet,c,lexicon):\n",
    "    s = bayes_log_prior(c)\n",
    "    for token in tweet:\n",
    "        if lexicon:\n",
    "            if token in lexicons['positive'] and c=='positive':\n",
    "                s = s + bayes_log_likelihood('{lexicon}',c)\n",
    "                continue\n",
    "            if token in lexicons['negative'] and c=='negative':\n",
    "                s = s + bayes_log_likelihood('{lexicon}',c)\n",
    "                continue\n",
    "            if token in lexicons['neutral'] and c=='neutral':\n",
    "                s = s + bayes_log_likelihood('{lexicon}',c)\n",
    "                continue\n",
    "        if token in vocabulary:\n",
    "            s = s + bayes_log_likelihood(token,c)\n",
    "    return s\n",
    "\n",
    "def bayes_predict(tweet,lexicon=False):\n",
    "    cat_key = {0:'positive',1:'negative',2:'neutral'}\n",
    "    likelihoods = [bayes_sum(tweet,c,lexicon) for c in ['positive','negative','neutral']]\n",
    "    return cat_key[likelihoods.index(max(likelihoods))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classes_for_svm(tweetgts):\n",
    "    return list(map(lambda a: 0 if a=='negative' else 1 if a=='neutral' else 2 if a=='positive' else None,tweetgts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt','r',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    l = line.split()\n",
    "    lstm_word_embeddings[l[0]] = list(map(float,l[1:]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweet_length = max(map(len,tweets_preprocessed['twitter-training-data.txt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_embed_tweet(self, tweet):\n",
    "    tweet_embedding = list(map(lambda a:lstm_word_embeddings[a],tweet))\n",
    "    for i in range(max_tweet_length+1-len(tweet_embedding)):\n",
    "        tweet_embedding.append([0]*100)\n",
    "        #print(tweet_embedding)\n",
    "    return FloatTensor(tweet_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(vocabulary):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSemtimentClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(LSTMSemtimentClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \"\"\"\n",
    "        self.embedding_layer = nn.Embedding(len(vocabulary), 100)\n",
    "        self.embedding_layer.load_state_dict({'weight': weights_matrix})\n",
    "        self.embedding_layer.weight.requires_grad = False\n",
    "        \"\"\"\n",
    "        self.lstm = nn.LSTM(100, hidden_dim)\n",
    "\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, 3)\n",
    "\n",
    "    def forward(self, tweet):\n",
    "        embeds = self.embed_tweet(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LSTMSemtimentClassifier(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training naive_bayes\n",
      "semeval-tweets\\twitter-dev-data.txt (bow-naive_bayes): 0.598\n",
      "semeval-tweets\\twitter-test1.txt (bow-naive_bayes): 0.486\n",
      "semeval-tweets\\twitter-test2.txt (bow-naive_bayes): 0.468\n",
      "semeval-tweets\\twitter-test3.txt (bow-naive_bayes): 0.485\n",
      "semeval-tweets\\twitter-dev-data.txt (bow+lexicons-naive_bayes): 0.562\n",
      "semeval-tweets\\twitter-test1.txt (bow+lexicons-naive_bayes): 0.512\n",
      "semeval-tweets\\twitter-test2.txt (bow+lexicons-naive_bayes): 0.506\n",
      "semeval-tweets\\twitter-test3.txt (bow+lexicons-naive_bayes): 0.508\n",
      "Training svm\n",
      "semeval-tweets\\twitter-dev-data.txt (svm): 0.577\n",
      "semeval-tweets\\twitter-test1.txt (svm): 0.498\n",
      "semeval-tweets\\twitter-test2.txt (svm): 0.522\n",
      "semeval-tweets\\twitter-test3.txt (svm): 0.495\n"
     ]
    }
   ],
   "source": [
    "svmoutputtoclass = lambda a: 'negative' if a==0 else 'neutral' if a==1 else 'positive' if a==2 else None\n",
    "\n",
    "for classifier in ['naive_bayes', 'svm']:\n",
    "\n",
    "    print('Training',classifier)\n",
    "        \n",
    "    if classifier=='naive_bayes':\n",
    "        #continue \n",
    "        for features in ['bow','bow+lexicons']:\n",
    "            for testset in ['twitter-dev-data.txt']+testsets:\n",
    "                id_preds = {}\n",
    "                vocabulary = set()\n",
    "                vocabulary_counts_positive = {}\n",
    "                vocabulary_counts_negative = {}\n",
    "                vocabulary_counts_neutral = {}\n",
    "                bayes_train(tweetgts['twitter-training-data.txt'],tweets_preprocessed['twitter-training-data.txt'],\n",
    "                            lexicon=True if features=='bow+lexicons' else False)\n",
    "                for tweetid,tweet in zip(tweetids[testset],tweets_preprocessed[testset]):\n",
    "                    id_preds[tweetid] = bayes_predict(tweet,lexicon=True if features=='bow+lexicons' else False)   \n",
    "                testset_name = testset\n",
    "                testset_path = join('semeval-tweets', testset_name)\n",
    "                evaluate(id_preds, testset_path, features+'-'+classifier)\n",
    "            \n",
    "    if classifier=='svm':\n",
    "        #continue\n",
    "        vectorizer = TfidfVectorizer(min_df = 5,max_df = 0.8,sublinear_tf = True,use_idf = True)\n",
    "        train_vectors = vectorizer.fit_transform(tweets_preprocessed_not_split['twitter-training-data.txt'])\n",
    "        #uncomment this to build SVM model instead of loading from pickle\n",
    "        #clf = svm.SVC()\n",
    "        #clf.fit(train_vectors, classes_for_svm(tweetgts['twitter-training-data.txt']))\n",
    "        clf = pickle.load( open( \"svmmodel.p\", \"rb\" ) )\n",
    "        for testset in ['twitter-dev-data.txt']+testsets:\n",
    "            id_preds = {}\n",
    "            test_vectors = vectorizer.transform(tweets_preprocessed_not_split[testset])\n",
    "            dev_set_predictions = clf.predict(test_vectors)\n",
    "            for i in range(len(tweetids[testset])):              \n",
    "                id_preds[tweetids[testset][i]] = svmoutputtoclass(dev_set_predictions[i])\n",
    "    \n",
    "            testset_name = testset\n",
    "            testset_path = join('semeval-tweets', testset_name)\n",
    "            evaluate(id_preds, testset_path, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
